"""
Mock provider for testing purposes.
This provider simulates API responses without making actual API calls.
"""

import asyncio
import json
import random
import time
from collections.abc import AsyncGenerator
from typing import Any

# Constants
MAX_PREVIEW_LENGTH = 20  # Maximum length for preview text before truncating

fake_responses = {
    "chat": [
        "This is a mock response from the mock provider. It simulates a real response without making API calls.",
        "The mock provider is useful for testing without consuming API credits or requiring real API keys.",
        "You can use the mock provider in CI/CD pipelines, development environments, or for demo purposes.",
        "This text was generated by the mock provider, not by an actual AI model.",
    ],
    "completion": [
        "Mock completion response for testing purposes. This text wasn't generated by a real AI model.",
        "This is simulated output from the mock provider, useful for testing without API calls.",
        "The mock provider helps test your application's integration with Forge without real API keys.",
        "In a production environment, this would be actual AI-generated text based on your prompt.",
    ],
}


def get_mock_chat_completion(
    model: str,
    messages: list[dict[str, str]],
    temperature: float = 0.7,
    stream: bool = False,
    **kwargs,
) -> dict[str, Any]:
    """Generate a mock chat completion response"""

    # Simulate processing time
    time.sleep(0.5)

    # Get the last message content for context (just for show)
    last_message = messages[-1]["content"] if messages else "No input provided"

    # Pick a random response
    response_text = random.choice(fake_responses["chat"])

    # Add a reference to the input to make it feel more responsive
    shortened_input = (
        last_message[:MAX_PREVIEW_LENGTH] + "..."
        if len(last_message) > MAX_PREVIEW_LENGTH
        else last_message
    )
    response_text = f"You asked: '{shortened_input}'\n\n{response_text}"

    # Create a response in the expected format
    response = {
        "id": f"mockresponse-{int(time.time())}",
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_text,
                },
                "finish_reason": "stop",
            }
        ],
        "usage": {
            "prompt_tokens": sum(len(m.get("content", "")) for m in messages) // 4,
            "completion_tokens": len(response_text) // 4,
            "total_tokens": (
                sum(len(m.get("content", "")) for m in messages) + len(response_text)
            )
            // 4,
        },
    }

    return response


def get_mock_text_completion(
    model: str, prompt: str, temperature: float = 0.7, stream: bool = False, **kwargs
) -> dict[str, Any]:
    """Generate a mock text completion response"""

    # Simulate processing time
    time.sleep(0.5)

    # Pick a random response
    response_text = random.choice(fake_responses["completion"])

    # Add a reference to the input to make it feel more responsive
    shortened_input = (
        prompt[:MAX_PREVIEW_LENGTH] + "..."
        if len(prompt) > MAX_PREVIEW_LENGTH
        else prompt
    )
    response_text = f"You asked: '{shortened_input}'\n\n{response_text}"

    # Create a response in the expected format
    response = {
        "id": f"mockresponse-{int(time.time())}",
        "object": "text_completion",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "text": response_text,
                "index": 0,
                "logprobs": None,
                "finish_reason": "stop",
            }
        ],
        "usage": {
            "prompt_tokens": len(prompt) // 4,
            "completion_tokens": len(response_text) // 4,
            "total_tokens": (len(prompt) + len(response_text)) // 4,
        },
    }

    return response


def get_mock_models() -> list[dict[str, Any]]:
    """Return a list of mock models"""
    mock_models = [
        {
            "id": "mock-only-gpt-3.5-turbo",
            "object": "model",
            "created": int(time.time()) - 86400 * 30,  # 30 days ago
            "owned_by": "mock-provider",
        },
        {
            "id": "mock-only-gpt-4",
            "object": "model",
            "created": int(time.time()) - 86400 * 30,  # 30 days ago
            "owned_by": "mock-provider",
        },
        {
            "id": "mock-only-gpt-4o",
            "object": "model",
            "created": int(time.time()) - 86400 * 7,  # 7 days ago
            "owned_by": "mock-provider",
        },
        {
            "id": "mock-only-claude-3-opus",
            "object": "model",
            "created": int(time.time()) - 86400 * 14,  # 14 days ago
            "owned_by": "mock-provider",
        },
        {
            "id": "mock-only-claude-3-sonnet",
            "object": "model",
            "created": int(time.time()) - 86400 * 14,  # 14 days ago
            "owned_by": "mock-provider",
        },
        {
            "id": "mock-only-claude-3-haiku",
            "object": "model",
            "created": int(time.time()) - 86400 * 14,  # 14 days ago
            "owned_by": "mock-provider",
        },
    ]
    return mock_models


async def generate_mock_chat_stream(
    model: str, messages: list[dict[str, str]], **kwargs
) -> AsyncGenerator[str, None]:
    """Generate a streaming mock response for chat completions"""
    request_id = f"mockstream-{int(time.time())}"

    # Initial response with role
    yield json.dumps(
        {
            "id": request_id,
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model,
            "choices": [
                {"index": 0, "delta": {"role": "assistant"}, "finish_reason": None}
            ],
        }
    )

    await asyncio.sleep(0.1)

    # Get a mock response first
    response = get_mock_chat_completion(model=model, messages=messages)
    content = response["choices"][0]["message"]["content"]
    words = content.split()

    # Stream word by word with small delays
    for i in range(0, len(words), 2):
        chunk = (
            " ".join(words[i : i + 2]) + " " if i + 1 < len(words) else words[i] + " "
        )
        yield json.dumps(
            {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": model,
                "choices": [
                    {"index": 0, "delta": {"content": chunk}, "finish_reason": None}
                ],
            }
        )
        await asyncio.sleep(0.1)

    # Final response
    yield json.dumps(
        {
            "id": request_id,
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model,
            "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
        }
    )

    yield "[DONE]"


class MockClient:
    """
    Mock client that simulates OpenAI's client for testing.
    This can be used as a drop-in replacement for testing without real API calls.
    """

    def __init__(self, api_key: str = "mock-api-key", base_url: str = "http://mock"):
        self.api_key = api_key
        self.base_url = base_url

    async def chat_completions_create(
        self, model: str, messages: list[dict[str, str]], **kwargs
    ):
        """Create a chat completion (compatible with OpenAI's client)"""
        stream = kwargs.get("stream", False)

        if stream:
            # Return a streaming object
            return MockChatCompletionStream(model, messages, **kwargs)

        # Return a regular response
        return MockChatCompletion(get_mock_chat_completion(model, messages, **kwargs))

    async def completions_create(self, model: str, prompt: str, **kwargs):
        """Create a text completion (compatible with OpenAI's client)"""
        return MockCompletion(get_mock_text_completion(model, prompt, **kwargs))

    async def models_list(self):
        """List available models (compatible with OpenAI's client)"""
        return MockModelList(get_mock_models())


class MockChatCompletion:
    """Mock ChatCompletion response object"""

    def __init__(self, response_data: dict[str, Any]):
        self._response = response_data
        for key, value in response_data.items():
            if key == "choices" and isinstance(value, list):
                # Special handling for choices to make them accessible via attributes
                self.choices = [MockChatCompletionChoice(choice) for choice in value]
            else:
                setattr(self, key, value)

    def __getattr__(self, attr):
        if attr in self._response:
            return self._response[attr]
        raise AttributeError(f"'MockChatCompletion' has no attribute '{attr}'")

    def __getitem__(self, key):
        return self._response[key]

    def model_dump(self):
        """Return the response as a dict (compatible with OpenAI's client)"""
        return self._response

    def model_dump_json(self):
        """Return the response as JSON (compatible with OpenAI's client)"""
        return json.dumps(self._response)


class MockChatCompletionChoice:
    """Mock ChatCompletionChoice object for nested access"""

    def __init__(self, choice_data: dict[str, Any]):
        self._data = choice_data
        for key, value in choice_data.items():
            if key == "message" and isinstance(value, dict):
                # Special handling for message to make it accessible via attributes
                self.message = MockChatMessage(value)
            else:
                setattr(self, key, value)

    def __getattr__(self, attr):
        if attr in self._data:
            return self._data[attr]
        raise AttributeError(f"'MockChatCompletionChoice' has no attribute '{attr}'")

    def __getitem__(self, key):
        return self._data[key]


class MockChatMessage:
    """Mock ChatMessage object for nested access"""

    def __init__(self, message_data: dict[str, Any]):
        self._data = message_data
        for key, value in message_data.items():
            setattr(self, key, value)

    def __getattr__(self, attr):
        if attr in self._data:
            return self._data[attr]
        raise AttributeError(f"'MockChatMessage' has no attribute '{attr}'")

    def __getitem__(self, key):
        return self._data[key]


class MockCompletion:
    """Mock Completion response object"""

    def __init__(self, response_data: dict[str, Any]):
        self._response = response_data
        for key, value in response_data.items():
            if key == "choices" and isinstance(value, list):
                # Special handling for choices to make them accessible via attributes
                self.choices = [MockCompletionChoice(choice) for choice in value]
            else:
                setattr(self, key, value)

    def __getattr__(self, attr):
        if attr in self._response:
            return self._response[attr]
        raise AttributeError(f"'MockCompletion' has no attribute '{attr}'")

    def __getitem__(self, key):
        return self._response[key]

    def model_dump(self):
        """Return the response as a dict (compatible with OpenAI's client)"""
        return self._response

    def model_dump_json(self):
        """Return the response as JSON (compatible with OpenAI's client)"""
        return json.dumps(self._response)


class MockCompletionChoice:
    """Mock CompletionChoice object for nested access"""

    def __init__(self, choice_data: dict[str, Any]):
        self._data = choice_data
        for key, value in choice_data.items():
            if key == "text" and isinstance(value, str):
                # Special handling for text to make it accessible via attributes
                self.text = value
            else:
                setattr(self, key, value)

    def __getattr__(self, attr):
        if attr in self._data:
            return self._data[attr]
        raise AttributeError(f"'MockCompletionChoice' has no attribute '{attr}'")

    def __getitem__(self, key):
        return self._data[key]


class MockModelList:
    """Mock Models response object"""

    def __init__(self, models: list[dict[str, Any]]):
        # Convert dictionaries to objects with attribute access
        self.data = [MockModel(model) for model in models]
        self.object = "list"

    def __iter__(self):
        return iter(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

    def model_dump(self):
        """Return the response as a dict (compatible with OpenAI's client)"""
        return {
            "object": self.object,
            "data": [model.model_dump() for model in self.data],
        }


class MockModel:
    """Mock Model object with attribute access"""

    def __init__(self, model_data: dict[str, Any]):
        self._data = model_data
        for key, value in model_data.items():
            setattr(self, key, value)

    def __getattr__(self, attr):
        if attr in self._data:
            return self._data[attr]
        raise AttributeError(f"'MockModel' has no attribute '{attr}'")

    def __getitem__(self, key):
        return self._data[key]

    def model_dump(self):
        """Return the model as a dict"""
        return self._data


class MockChatCompletionStream:
    """Mock streaming chat completion response"""

    def __init__(self, model: str, messages: list[dict[str, str]], **kwargs):
        self.model = model
        self.messages = messages
        self.kwargs = kwargs

    async def __aiter__(self):
        """Async iterator that yields chunks of the response"""
        async for chunk_text in generate_mock_chat_stream(
            self.model, self.messages, **self.kwargs
        ):
            if chunk_text == "[DONE]":
                break

            chunk_data = json.loads(chunk_text)
            yield MockChatCompletionChunk(chunk_data)


class MockChatCompletionChunk:
    """Mock chat completion chunk for streaming"""

    def __init__(self, chunk_data: dict[str, Any]):
        self._data = chunk_data
        for key, value in chunk_data.items():
            if key == "choices" and isinstance(value, list):
                # Special handling for choices to make them accessible via attributes
                self.choices = [
                    MockChatCompletionChunkDelta(choice) for choice in value
                ]
            else:
                setattr(self, key, value)

    def __getattr__(self, attr):
        if attr in self._data:
            return self._data[attr]
        raise AttributeError(f"'MockChatCompletionChunk' has no attribute '{attr}'")

    def __getitem__(self, key):
        return self._data[key]

    def model_dump(self):
        """Return the chunk as a dict (compatible with OpenAI's client)"""
        return self._data


class MockChatCompletionChunkDelta:
    """Mock chat completion chunk delta for nested attribute access"""

    def __init__(self, delta_data: dict[str, Any]):
        self._data = delta_data
        for key, value in delta_data.items():
            if key == "delta" and isinstance(value, dict):
                # Special handling for delta to make it accessible via attributes
                self.delta = type("MockDelta", (), value)
            else:
                setattr(self, key, value)

    def __getattr__(self, attr):
        if attr in self._data:
            return self._data[attr]
        raise AttributeError(
            f"'MockChatCompletionChunkDelta' has no attribute '{attr}'"
        )

    def __getitem__(self, key):
        return self._data[key]
